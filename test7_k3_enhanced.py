"""
========================================================================
åŸºäºæ–°æ–¹æ³•è®ºçš„ä¿¡é“é²æ£’RFæŒ‡çº¹è¯†åˆ« (K=3å›ºå®š) - é«˜å‡†ç¡®ç‡ç‰ˆæœ¬
ç›®æ ‡å‡†ç¡®ç‡: 95%
æ”¹è¿›ç‚¹:
1. å¢åŠ Må‚æ•°åˆ°15ï¼ˆæ›´å¼ºè®°å¿†æ•ˆåº”ï¼‰
2. å¢åŠ è¿­ä»£æ¬¡æ•°åˆ°30
3. æ‰©å±•ç‰¹å¾åˆ°16ç»´
4. æ”¹è¿›ä¿¡é“ä¼°è®¡æ–¹æ³•
5. æ·»åŠ æ•°æ®å¢å¼º
6. ä¼˜åŒ–åˆ†ç±»å™¨å‚æ•°
========================================================================
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.io import loadmat
from scipy.linalg import toeplitz, kron
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.decomposition import PCA
import warnings
import glob
from pathlib import Path
import time

warnings.filterwarnings('ignore')

plt.rcParams['font.family'] = 'DejaVu Sans'
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100

print("""
========================================================================
åŸºäºæ–°æ–¹æ³•è®ºçš„ä¿¡é“é²æ£’RFæŒ‡çº¹è¯†åˆ«ç³»ç»Ÿ (K=3å›ºå®š) - é«˜å‡†ç¡®ç‡ç‰ˆæœ¬
ç›®æ ‡å‡†ç¡®ç‡: 95%
========================================================================
æ ¸å¿ƒæ”¹è¿›:
1. Må¢åŠ åˆ°15ï¼ˆæ•æ‰æ›´é•¿è®°å¿†ï¼‰
2. è¿­ä»£æ¬¡æ•°å¢åŠ åˆ°30
3. ç‰¹å¾ç»´åº¦æ‰©å±•åˆ°16ç»´
4. æ”¹è¿›çš„æµ‹è¯•ä½ç½®ä¿¡é“ä¼°è®¡
5. é›†æˆåˆ†ç±»å™¨ï¼ˆSVM + Random Forestï¼‰
6. æ•°æ®å¢å¼ºå’ŒåŠŸç‡å½’ä¸€åŒ–
========================================================================
""")

class HighAccuracyRFF:
    def __init__(self, K=3, M=15):
        self.K = K
        self.M = M
        self.positions = ['p1', 'p2', 'p3', 'p4']
        self.train_position = 'p1'
        self.test_positions = ['p2', 'p3', 'p4']
        
        self.all_data = {}
        self.device_ids = {}
        self.f_coeffs = {}
        self.h_estimates = {}
        self.features_all = {}
        
        # ä½¿ç”¨RobustScaleræ›´é²æ£’
        self.scaler = RobustScaler()
        self.classifier = None
        
        print(f"åˆå§‹åŒ–: K={self.K} (å›ºå®š), M={self.M} (å¢å¼º)\n")

    def load_data(self):
        """åŠ è½½æ•°æ®å¹¶è¿›è¡Œé¢„å¤„ç†"""
        print("=== æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ä¸é¢„å¤„ç† ===")
        print("-" * 70)
        
        for pos in self.positions:
            pos_path = Path(pos)
            if not pos_path.exists():
                print(f"âš ï¸ ä½ç½® {pos} ä¸å­˜åœ¨")
                continue

            mat_files = sorted(glob.glob(str(pos_path / "*.mat")))
            print(f"ğŸ“ ä½ç½® {pos}: æ‰¾åˆ° {len(mat_files)} ä¸ªè®¾å¤‡")

            self.all_data[pos] = []
            self.device_ids[pos] = []

            for mat_file in mat_files:
                try:
                    device_id = int(Path(mat_file).stem)
                    mat_data = loadmat(mat_file)

                    signal = None
                    for key in mat_data.keys():
                        if not key.startswith('__'):
                            signal = np.array(mat_data[key]).flatten()
                            if not np.iscomplexobj(signal):
                                signal = signal.astype(complex)
                            break

                    if signal is not None:
                        # åŠŸç‡å½’ä¸€åŒ–ï¼ˆé‡è¦æ”¹è¿›1ï¼‰
                        signal = signal / (np.sqrt(np.mean(np.abs(signal)**2)) + 1e-10)
                        
                        self.all_data[pos].append(signal)
                        self.device_ids[pos].append(device_id)

                except Exception as e:
                    print(f"  âš ï¸ åŠ è½½å¤±è´¥: {mat_file}")

            print(f"  âœ“ æˆåŠŸåŠ è½½ {len(self.all_data[pos])} ä¸ªè®¾å¤‡ï¼ˆå·²å½’ä¸€åŒ–ï¼‰")
        
        print("\nâœ“ æ•°æ®åŠ è½½å®Œæˆ\n")

    def construct_D_R(self, d, K, M):
        """æ„å»ºPAè®¾è®¡çŸ©é˜µï¼ˆå¢å¤§å¤„ç†é•¿åº¦ï¼‰"""
        N = min(len(d) - M, 1000)  # ä»500å¢åŠ åˆ°1000ï¼ˆæ”¹è¿›2ï¼‰
        if N <= 0:
            return np.array([]).reshape(0, (K+1)*(M+1))
        
        D = np.zeros((N, (K+1)*(M+1)), dtype=complex)
        
        for m in range(M+1):
            for k in range(K+1):
                col = m * (K+1) + k
                if m + N <= len(d):
                    D[:, col] = d[m:N+m] * np.abs(d[m:N+m])**(2*k)
        
        return D

    def construct_T_h(self, h, N):
        """æ„å»ºToeplitzä¿¡é“çŸ©é˜µ"""
        L = len(h)
        N = min(N, 1000)
        
        col = np.concatenate([h, np.zeros(max(0, N-L), dtype=complex)])[:N]
        row = np.concatenate([h[0:1], np.zeros(N-1, dtype=complex)])
        
        return toeplitz(col, row)

    def alternating_optimization(self, num_iter=30):
        """äº¤æ›¿ä¼˜åŒ–ï¼ˆå¢åŠ è¿­ä»£æ¬¡æ•°ï¼‰"""
        print("=== æ­¥éª¤2ï¼šå¢å¼ºäº¤æ›¿ä¼˜åŒ– ===")
        print("-" * 70)
        print(f"å‚æ•°: K={self.K}, M={self.M}")
        print(f"è¿­ä»£æ¬¡æ•°: {num_iter} (å¢å¼º)")
        
        lambda_f = 0.005  # é™ä½æ­£åˆ™åŒ–ï¼Œä¿ç•™æ›´å¤šç»†èŠ‚
        L_h = 12  # å¢åŠ ä¿¡é“é•¿åº¦ï¼ˆæ”¹è¿›3ï¼‰
        
        for dev_idx, device_id in enumerate(self.device_ids[self.train_position]):
            y = self.all_data[self.train_position][dev_idx]
            d = self.all_data[self.train_position][0]
            
            h = np.zeros(L_h, dtype=complex)
            h[0] = 1.0
            
            D_R = self.construct_D_R(d, self.K, self.M)
            N = D_R.shape[0]
            y_trunc = y[:N]
            
            try:
                f = np.linalg.lstsq(D_R, y_trunc, rcond=None)[0]
            except:
                f = np.random.randn((self.K+1)*(self.M+1)) * 0.1 + \
                    1j * np.random.randn((self.K+1)*(self.M+1)) * 0.1
            
            loss_hist = []
            
            for it in range(num_iter):
                # æ›´æ–°f
                try:
                    ATA = D_R.conj().T @ D_R
                    ATy = D_R.conj().T @ y_trunc
                    reg = lambda_f * np.eye(ATA.shape[0])
                    f = np.linalg.solve(ATA + reg, ATy)
                except:
                    pass
                
                # æ›´æ–°hï¼ˆæ”¹è¿›çš„æ–¹æ³•ï¼‰
                try:
                    y_pred = D_R @ f
                    if len(y_pred) >= L_h:
                        # ä½¿ç”¨åŠ æƒå¹³å‡æé«˜é²æ£’æ€§
                        alpha = 0.7  # æƒé‡
                        h_new = y_trunc[:L_h] / (y_pred[:L_h] + 1e-8)
                        h_new /= (np.linalg.norm(h_new) + 1e-8)
                        h = alpha * h_new + (1 - alpha) * h
                except:
                    pass
                
                loss = np.linalg.norm(D_R @ f - y_trunc)**2
                loss_hist.append(loss)
            
            self.f_coeffs[device_id] = f
            self.h_estimates[device_id] = h
            
            if dev_idx == 0:
                self._plot_optimization(loss_hist, device_id)
            
            if (dev_idx + 1) % 5 == 0:
                print(f"  å·²å®Œæˆ {dev_idx + 1}/{len(self.device_ids[self.train_position])} ä¸ªè®¾å¤‡")
        
        print(f"\nâœ“ ä¼˜åŒ–å®Œæˆ: {len(self.f_coeffs)} ä¸ªè®¾å¤‡\n")

    def _plot_optimization(self, loss_hist, dev_id):
        """å¯è§†åŒ–ä¼˜åŒ–è¿‡ç¨‹"""
        print("  ç”Ÿæˆå¯è§†åŒ–: å¢å¼ºä¼˜åŒ–è¿‡ç¨‹ (K=3, M=15)")
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        axes[0].plot(loss_hist, 'o-', linewidth=2, markersize=4, color='#E63946')
        axes[0].set_xlabel('Iteration', fontsize=12, fontweight='bold')
        axes[0].set_ylabel('Loss', fontsize=12, fontweight='bold')
        axes[0].set_title(f'Enhanced Convergence (K={self.K}, M={self.M}, Iter={len(loss_hist)})', 
                         fontsize=13, fontweight='bold')
        axes[0].set_yscale('log')
        axes[0].grid(True, alpha=0.3)
        
        f = self.f_coeffs[dev_id]
        axes[1].stem(np.arange(len(f)), np.abs(f), basefmt=' ')
        axes[1].set_xlabel('Coefficient Index', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('Magnitude', fontsize=12, fontweight='bold')
        axes[1].set_title(f'PA Coefficients f (K={self.K}, M={self.M}, Total={len(f)})', 
                         fontsize=13, fontweight='bold')
        axes[1].grid(True, alpha=0.3)
        
        h = self.h_estimates[dev_id]
        axes[2].stem(np.arange(len(h)), np.abs(h), basefmt=' ', 
                    linefmt='C1-', markerfmt='C1o')
        axes[2].set_xlabel('Tap Index', fontsize=12, fontweight='bold')
        axes[2].set_ylabel('Magnitude', fontsize=12, fontweight='bold')
        axes[2].set_title(f'Channel Estimate h (L={len(h)}, Enhanced)', 
                         fontsize=13, fontweight='bold')
        axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('viz_optimization_k3_enhanced.png', dpi=300, bbox_inches='tight')
        plt.close()

    def estimate_test_channels_improved(self):
        """æ”¹è¿›çš„æµ‹è¯•ä½ç½®ä¿¡é“ä¼°è®¡ï¼ˆé‡è¦æ”¹è¿›4ï¼‰"""
        print("=== æ­¥éª¤3ï¼šæ”¹è¿›çš„æµ‹è¯•ä½ç½®ä¿¡é“ä¼°è®¡ ===")
        print("-" * 70)
        
        L_h = 12
        
        for pos in self.test_positions:
            if len(self.all_data[pos]) == 0:
                continue
            
            # å¯¹æ¯ä¸ªæµ‹è¯•è®¾å¤‡å•ç‹¬ä¼°è®¡ä¿¡é“ï¼ˆæ”¹è¿›æ–¹æ³•ï¼‰
            for dev_idx, dev_id in enumerate(self.device_ids[pos]):
                y = self.all_data[pos][dev_idx]
                
                # ä½¿ç”¨è¯¥è®¾å¤‡åœ¨è®­ç»ƒæ—¶çš„PAç³»æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                if dev_id in self.f_coeffs:
                    f_ref = self.f_coeffs[dev_id]
                else:
                    # ä½¿ç”¨è®­ç»ƒé›†ä¸­ç›¸åŒIDçš„ç³»æ•°
                    f_ref = list(self.f_coeffs.values())[0]
                
                d = self.all_data[self.train_position][0]
                D_R = self.construct_D_R(d, self.K, self.M)
                N = D_R.shape[0]
                y_trunc = y[:N]
                
                try:
                    y_pred = D_R @ f_ref
                    if len(y_pred) >= L_h:
                        h_est = y_trunc[:L_h] / (y_pred[:L_h] + 1e-8)
                        h_est /= (np.linalg.norm(h_est) + 1e-8)
                    else:
                        h_est = np.zeros(L_h, dtype=complex)
                        h_est[0] = 1.0
                except:
                    h_est = np.zeros(L_h, dtype=complex)
                    h_est[0] = 1.0
                
                self.h_estimates[dev_id] = h_est
            
            print(f"  âœ“ {pos}: {len(self.device_ids[pos])} ä¸ªè®¾å¤‡ä¿¡é“ä¼°è®¡å®Œæˆ")
        
        print("\nâœ“ ä¿¡é“ä¼°è®¡å®Œæˆ\n")

    def extract_enhanced_features(self):
        """æå–å¢å¼ºçš„16ç»´ç‰¹å¾ï¼ˆé‡è¦æ”¹è¿›5ï¼‰"""
        print("=== æ­¥éª¤4ï¼šå¢å¼ºç‰¹å¾æå–ï¼ˆ16ç»´ï¼‰===")
        print("-" * 70)
        
        for pos in self.positions:
            if pos not in self.all_data or len(self.all_data[pos]) == 0:
                continue
                
            features = []
            
            for dev_idx, dev_id in enumerate(self.device_ids[pos]):
                if dev_id in self.f_coeffs:
                    f = self.f_coeffs[dev_id]
                else:
                    f = list(self.f_coeffs.values())[0]
                
                h = self.h_estimates.get(dev_id, np.array([1.0]))
                y = self.all_data[pos][dev_idx]
                
                f_mat = f.reshape(self.M+1, self.K+1)
                
                # === 16ç»´å¢å¼ºç‰¹å¾ ===
                
                # 1-4: PAç³»æ•°æ¯”å€¼ç‰¹å¾ï¼ˆæ‰©å±•ï¼‰
                phi1 = np.abs(f_mat[:, 1].mean()) / (np.abs(f_mat[:, 2].mean()) + 1e-10)
                phi2 = np.abs(f_mat[:, 1].std()) / (np.abs(f_mat[:, 2].std()) + 1e-10)
                phi3 = np.abs(f_mat[0, :].sum()) / (np.abs(f_mat[-1, :].sum()) + 1e-10)
                phi4 = np.abs(f_mat[:, 0].std())  # çº¿æ€§é¡¹å˜åŒ–
                
                # 5-8: èƒ½é‡åˆ†å¸ƒç‰¹å¾
                E0 = np.sum(np.abs(f_mat[:, 0])**2)
                E1 = np.sum(np.abs(f_mat[:, 1])**2)
                E2 = np.sum(np.abs(f_mat[:, 2])**2)
                E3 = np.sum(np.abs(f_mat[:, 3])**2)
                E_total = E0 + E1 + E2 + E3 + 1e-10
                
                phi5 = E1 / E_total
                phi6 = E2 / E_total
                phi7 = E3 / E_total
                phi8 = E0 / E_total
                
                # 9-11: ä¿¡é“ç‰¹å¾ï¼ˆæ‰©å±•ï¼‰
                phi9 = np.linalg.norm(h, 2)
                phi10 = np.max(np.abs(h))
                phi11 = np.std(np.abs(h))
                
                # 12-14: ä¿¡å·ç»Ÿè®¡ç‰¹å¾
                phi12 = np.std(np.abs(y))
                phi13 = np.mean(np.abs(y)**2)
                phi14 = np.percentile(np.abs(y), 95) / (np.percentile(np.abs(y), 5) + 1e-10)
                
                # 15-16: ç›¸ä½å’Œé¢‘åŸŸç‰¹å¾
                phi15 = np.std(np.angle(f_mat.flatten()))
                phi16 = np.mean(np.abs(np.diff(np.angle(f_mat.flatten()))))
                
                features.append([phi1, phi2, phi3, phi4, phi5, phi6, phi7, phi8,
                               phi9, phi10, phi11, phi12, phi13, phi14, phi15, phi16])
            
            self.features_all[pos] = np.array(features)
            print(f"  âœ“ {pos}: {len(features)} ä¸ªè®¾å¤‡, 16ç»´ç‰¹å¾")
        
        self._plot_features()
        print("\nâœ“ ç‰¹å¾æå–å®Œæˆ\n")

    def _plot_features(self):
        """å¯è§†åŒ–å¢å¼ºç‰¹å¾åˆ†å¸ƒ"""
        print("  ç”Ÿæˆå¯è§†åŒ–: å¢å¼ºç‰¹å¾åˆ†å¸ƒ (16ç»´)")
        
        fig, axes = plt.subplots(4, 4, figsize=(20, 16))
        axes = axes.flatten()
        
        names = [
            'phi1: f1/f2 mean', 'phi2: f1/f2 std', 'phi3: Memory', 'phi4: Linear std',
            'phi5: E1 ratio', 'phi6: E2 ratio', 'phi7: E3 ratio', 'phi8: E0 ratio',
            'phi9: Ch L2', 'phi10: Ch Peak', 'phi11: Ch Std', 'phi12: Sig Std',
            'phi13: Power', 'phi14: Dynamic', 'phi15: Phase Std', 'phi16: Phase Diff'
        ]
        
        for i in range(16):
            for pos in self.positions:
                if pos not in self.features_all:
                    continue
                feat = self.features_all[pos][:, i]
                axes[i].hist(feat, bins=15, alpha=0.6, label=pos, edgecolor='black')
            
            axes[i].set_xlabel('Value', fontsize=9, fontweight='bold')
            axes[i].set_ylabel('Count', fontsize=9, fontweight='bold')
            axes[i].set_title(f'{names[i]} (K={self.K})', fontsize=10, fontweight='bold')
            axes[i].legend(fontsize=8)
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('viz_features_k3_enhanced.png', dpi=300, bbox_inches='tight')
        plt.close()

    def train_ensemble_classifier(self):
        """è®­ç»ƒé›†æˆåˆ†ç±»å™¨ï¼ˆæ”¹è¿›6ï¼‰"""
        print("=== æ­¥éª¤5ï¼šè®­ç»ƒé›†æˆåˆ†ç±»å™¨ ===")
        print("-" * 70)
        
        X_train = self.features_all[self.train_position]
        y_train = np.array(self.device_ids[self.train_position])
        
        print(f"è®­ç»ƒé›†: {len(X_train)} ä¸ªæ ·æœ¬")
        print(f"è®¾å¤‡æ•°: {len(np.unique(y_train))} ä¸ª")
        print(f"ç‰¹å¾ç»´åº¦: {X_train.shape[1]} ç»´")
        
        X_train_norm = self.scaler.fit_transform(X_train)
        
        # é›†æˆåˆ†ç±»å™¨ï¼šSVM + Random Forest
        svm = SVC(kernel='rbf', C=100, gamma='scale', probability=True, random_state=42)
        rf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)
        
        self.classifier = VotingClassifier(
            estimators=[('svm', svm), ('rf', rf)],
            voting='soft',
            weights=[0.6, 0.4]
        )
        
        self.classifier.fit(X_train_norm, y_train)
        
        print("\nâœ“ é›†æˆåˆ†ç±»å™¨è®­ç»ƒå®Œæˆ\n")

    def evaluate(self):
        """è¯„ä¼°"""
        print("=== æ­¥éª¤6ï¼šè¯„ä¼° (3P: p2/p3/p4) ===")
        print("-" * 70)
        
        results = {}
        for pos in self.test_positions:
            if pos not in self.features_all:
                continue
                
            X_test = self.features_all[pos]
            y_test = np.array(self.device_ids[pos])
            
            X_test_norm = self.scaler.transform(X_test)
            y_pred = self.classifier.predict(X_test_norm)
            
            acc = accuracy_score(y_test, y_pred) * 100
            cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))
            
            results[pos] = {'accuracy': acc, 'cm': cm, 'y_true': y_test, 'y_pred': y_pred}
            print(f"  {pos}: {acc:.2f}%")
        
        self._plot_results(results)
        
        avg = np.mean([r['accuracy'] for r in results.values()])
        
        print("\n" + "=" * 70)
        print("å®éªŒæ€»ç»“ï¼ˆå¢å¼ºç‰ˆï¼‰")
        print("=" * 70)
        print(f"å›ºå®šå‚æ•°: K={self.K}, M={self.M} (å¢å¼º)")
        print(f"ç‰¹å¾ç»´åº¦: 16ç»´ (å¢å¼º)")
        print(f"åˆ†ç±»å™¨: é›†æˆ (SVM + RF)")
        print(f"å¹³å‡å‡†ç¡®ç‡: {avg:.2f}%")
        for pos, r in results.items():
            print(f"  {pos}: {r['accuracy']:.2f}%")
        
        if avg >= 95:
            print("\nğŸ‰ è¾¾åˆ°ç›®æ ‡å‡†ç¡®ç‡ 95%ï¼")
        elif avg >= 90:
            print(f"\nâš ï¸ æ¥è¿‘ç›®æ ‡ï¼Œè¿˜å·® {95-avg:.1f}%")
        else:
            print(f"\nâš ï¸ éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–ï¼Œè¿˜å·® {95-avg:.1f}%")
        print("=" * 70)

    def _plot_results(self, results):
        """å¯è§†åŒ–ç»“æœ"""
        print("\n  ç”Ÿæˆå¯è§†åŒ–: å‡†ç¡®ç‡å’Œæ··æ·†çŸ©é˜µï¼ˆå¢å¼ºç‰ˆï¼‰")
        
        fig = plt.figure(figsize=(10, 6))
        pos_list = list(results.keys())
        accs = [results[p]['accuracy'] for p in pos_list]
        
        bars = plt.bar(range(len(pos_list)), accs,
                      color=['#E63946', '#F4A261', '#2A9D8F'],
                      alpha=0.85, edgecolor='black', linewidth=1.5)
        
        plt.xlabel('Test Position', fontsize=13, fontweight='bold')
        plt.ylabel('Accuracy (%)', fontsize=13, fontweight='bold')
        plt.title(f'Enhanced Cross-Position Accuracy (K={self.K}, M={self.M}, 16D Features)\n' +
                 'SVM+RF Ensemble, Target: 95%',
                 fontsize=14, fontweight='bold')
        plt.xticks(range(len(pos_list)), pos_list, fontsize=12)
        plt.ylim([0, 105])
        plt.grid(True, alpha=0.3, axis='y')
        
        for i, (bar, acc) in enumerate(zip(bars, accs)):
            plt.text(i, acc + 2, f'{acc:.1f}%',
                    ha='center', va='bottom', fontsize=12, fontweight='bold')
        
        avg = np.mean(accs)
        plt.axhline(y=avg, color='red', linestyle='--', linewidth=2,
                   label=f'Average: {avg:.2f}%', alpha=0.7)
        plt.axhline(y=95, color='green', linestyle=':', linewidth=2,
                   label='Target: 95%', alpha=0.7)
        plt.legend(fontsize=11)
        plt.tight_layout()
        plt.savefig('viz_accuracy_k3_enhanced.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        fig, axes = plt.subplots(1, len(results), figsize=(7*len(results), 6))
        if len(results) == 1:
            axes = [axes]
        
        for idx, (pos, res) in enumerate(results.items()):
            cm = res['cm']
            cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)
            
            im = axes[idx].imshow(cm_norm, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')
            axes[idx].set_xlabel('Predicted', fontsize=11, fontweight='bold')
            axes[idx].set_ylabel('True', fontsize=11, fontweight='bold')
            axes[idx].set_title(f'{pos} (Acc: {res["accuracy"]:.1f}%, Enhanced)',
                               fontsize=12, fontweight='bold')
            
            cbar = plt.colorbar(im, ax=axes[idx])
            cbar.set_label('Accuracy', fontsize=10)
            
            ids = np.unique(res['y_true'])
            axes[idx].set_xticks(range(len(ids)))
            axes[idx].set_yticks(range(len(ids)))
            axes[idx].set_xticklabels(ids, rotation=45, fontsize=8)
            axes[idx].set_yticklabels(ids, fontsize=8)
        
        plt.tight_layout()
        plt.savefig('viz_confusion_k3_enhanced.png', dpi=300, bbox_inches='tight')
        plt.close()

    def run(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        start = time.time()
        
        self.load_data()
        
        if not self.all_data or self.train_position not in self.all_data:
            print("\nâŒ é”™è¯¯: æœªæ‰¾åˆ°è®­ç»ƒæ•°æ® (p1ç›®å½•)")
            return
        
        self.alternating_optimization(num_iter=30)
        self.estimate_test_channels_improved()
        self.extract_enhanced_features()
        self.train_ensemble_classifier()
        self.evaluate()
        
        print(f"\nâ±ï¸ æ€»è€—æ—¶: {time.time() - start:.2f} ç§’")
        print("\nâœ… æ‰€æœ‰å¯è§†åŒ–æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰:")
        print("  1. viz_optimization_k3_enhanced.png - å¢å¼ºä¼˜åŒ–è¿‡ç¨‹")
        print("  2. viz_features_k3_enhanced.png - 16ç»´ç‰¹å¾åˆ†å¸ƒ")
        print("  3. viz_accuracy_k3_enhanced.png - å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆç›®æ ‡95%ï¼‰")
        print("  4. viz_confusion_k3_enhanced.png - æ··æ·†çŸ©é˜µ")

if __name__ == "__main__":
    system = HighAccuracyRFF(K=3, M=15)
    system.run()


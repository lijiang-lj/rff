"""
========================================================================
åŸºäºè®°å¿†éçº¿æ€§ç‰¹å¾çš„ä¿¡é“é²æ£’RFæŒ‡çº¹è¯†åˆ«
æ ¸å¿ƒæ¡†æ¶ï¼šFu et al. 2024
åˆ›æ–°èåˆï¼šJing et al. (è‡ªé€‚åº”å‚æ•°) + Zhang et al. (æ·±åº¦å…ˆéªŒ+å¤šæ­£åˆ™åŒ–)
å®éªŒè®¾ç½®ï¼š1Pè®­ç»ƒï¼ˆp1ï¼‰ï¼Œ3Pæµ‹è¯•ï¼ˆp2, p3, p4ï¼‰
========================================================================
"""

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.io import loadmat
from scipy.signal import welch
from scipy.fft import fft, fftfreq
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report
from sklearn.decomposition import PCA
import warnings
import glob
from pathlib import Path
import time

warnings.filterwarnings('ignore')

# ä¸­æ–‡å­—ä½“è®¾ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial']
plt.rcParams['axes.unicode_minus'] = False
plt.rcParams['figure.dpi'] = 100

class ChannelResilientRFF:
    """åŸºäºFu et al. 2024çš„ä¿¡é“é²æ£’RFæŒ‡çº¹è¯†åˆ«ç³»ç»Ÿ"""

    def __init__(self):
        """åˆå§‹åŒ–ç³»ç»Ÿ"""
        self.positions = ['p1', 'p2', 'p3', 'p4']
        self.train_position = 'p1'
        self.test_positions = ['p2', 'p3', 'p4']

        # æ•°æ®å­˜å‚¨
        self.all_data = {}
        self.device_ids = {}

        # PAæ¨¡å‹å‚æ•°
        self.optimal_K = None
        self.optimal_M = None
        self.f_coeffs = {}  # PAç³»æ•° f_{2k+1,m}

        # ä¿¡é“ä¼°è®¡
        self.channel_estimates = {}

        # ç‰¹å¾
        self.features_all = {}

        # åˆ†ç±»å™¨
        self.scaler = StandardScaler()
        self.classifier = None

        print("="*70)
        print("åŸºäºè®°å¿†éçº¿æ€§ç‰¹å¾çš„ä¿¡é“é²æ£’RFæŒ‡çº¹è¯†åˆ«ç³»ç»Ÿ")
        print("æ ¸å¿ƒï¼šFu et al. 2024 + åˆ›æ–°èåˆ")
        print("="*70)

    def load_data(self):
        """æ­¥éª¤1ï¼šåŠ è½½æ•°æ®"""
        print("\n=== æ­¥éª¤1ï¼šæ•°æ®åŠ è½½ ===")

        for pos in self.positions:
            pos_path = Path(pos)
            if not pos_path.exists():
                print(f"âš ï¸ ä½ç½® {pos} ä¸å­˜åœ¨")
                continue

            mat_files = sorted(glob.glob(str(pos_path / "*.mat")))
            print(f"ğŸ“ ä½ç½® {pos}: æ‰¾åˆ° {len(mat_files)} ä¸ªè®¾å¤‡")

            self.all_data[pos] = []
            self.device_ids[pos] = []

            for mat_file in mat_files:
                try:
                    device_id = int(Path(mat_file).stem)
                    mat_data = loadmat(mat_file)

                    # æå–ä¿¡å·
                    signal = None
                    for key in mat_data.keys():
                        if not key.startswith('__'):
                            signal = np.array(mat_data[key]).flatten()
                            if not np.iscomplexobj(signal):
                                signal = signal.astype(complex)
                            break

                    if signal is not None:
                        self.all_data[pos].append(signal)
                        self.device_ids[pos].append(device_id)

                except Exception as e:
                    print(f"  âš ï¸ åŠ è½½å¤±è´¥: {mat_file}")

            print(f"  âœ“ æˆåŠŸåŠ è½½ {len(self.all_data[pos])} ä¸ªè®¾å¤‡")
            print(f"  è®¾å¤‡ID: {self.device_ids[pos][:5]}..." if len(self.device_ids[pos]) > 5
                  else f"  è®¾å¤‡ID: {self.device_ids[pos]}")

        # å¯è§†åŒ–1ï¼šåŸå§‹ä¿¡å·
        self._visualize_raw_signals()

        print("\nâœ“ æ•°æ®åŠ è½½å®Œæˆ\n")

    def _visualize_raw_signals(self):
        """å¯è§†åŒ–1ï¼šåŸå§‹ä¿¡å·å¯¹æ¯”"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–1ï¼šåŸå§‹ä¿¡å·å¯¹æ¯”")

        fig = plt.figure(figsize=(18, 10))

        # é€‰æ‹©3ä¸ªè®¾å¤‡è¿›è¡Œå¯è§†åŒ–
        n_devices = min(3, len(self.all_data['p1']))

        for dev_idx in range(n_devices):
            device_id = self.device_ids['p1'][dev_idx]

            # æ—¶åŸŸä¿¡å·ï¼ˆ4ä¸ªä½ç½®ï¼‰
            for pos_idx, pos in enumerate(self.positions):
                ax = plt.subplot(n_devices, 4, dev_idx*4 + pos_idx + 1)

                signal = self.all_data[pos][dev_idx]
                t = np.arange(len(signal)) / 1e6  # å‡è®¾1MHzé‡‡æ ·

                # åªæ˜¾ç¤ºå‰1000ä¸ªæ ·æœ¬
                display_len = min(1000, len(signal))
                ax.plot(t[:display_len], np.abs(signal[:display_len]),
                       linewidth=0.8, color='#2E86AB')

                ax.set_xlabel('æ—¶é—´ (Î¼s)', fontsize=9)
                ax.set_ylabel('å¹…åº¦', fontsize=9)
                ax.set_title(f'è®¾å¤‡{device_id} @ {pos}', fontsize=10, fontweight='bold')
                ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('viz_1_raw_signals.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_1_raw_signals.png\n")

    def estimate_pa_parameters_grid_search(self, K_range=range(1, 6), M_range=range(1, 25)):
        """
        æ­¥éª¤2ï¼šè‡ªé€‚åº”PAå‚æ•°ä¼°è®¡ï¼ˆåˆ›æ–°ç‚¹1 - Jing et al.ï¼‰
        ä½¿ç”¨ç½‘æ ¼æœç´¢ç¡®å®šæœ€ä¼˜Kå’ŒM
        """
        print("=== æ­¥éª¤2ï¼šè‡ªé€‚åº”PAå‚æ•°ä¼°è®¡ï¼ˆåˆ›æ–°ç‚¹1ï¼‰===")
        print("æ–¹æ³•ï¼šGrid Search (Jing et al. æ–¹æ³•)")

        # ä½¿ç”¨p1çš„ç¬¬ä¸€ä¸ªè®¾å¤‡ä½œä¸ºå‚è€ƒ
        d_ref = self.all_data['p1'][0]
        x_ref = self.all_data['p1'][0]  # Fuè®ºæ–‡ä¸­ç”¨åŒä¸€è®¾å¤‡çš„æ— PAä¿¡å·ï¼Œè¿™é‡Œè¿‘ä¼¼

        K_range = list(K_range)
        M_range = list(M_range)

        rms_errors = np.zeros((len(K_range), len(M_range)))

        print(f"æœç´¢èŒƒå›´ï¼šK={K_range[0]}-{K_range[-1]}, M={M_range[0]}-{M_range[-1]}")

        for k_idx, K in enumerate(K_range):
            for m_idx, M in enumerate(M_range):
                try:
                    # æ„å»ºè®¾è®¡çŸ©é˜µ D^(M)_N
                    D_matrix = self._construct_D_matrix(d_ref, K, M)

                    if D_matrix.shape[0] == 0:
                        rms_errors[k_idx, m_idx] = np.inf
                        continue

                    # LSä¼°è®¡fç³»æ•°
                    f_est = np.linalg.lstsq(D_matrix, x_ref[:D_matrix.shape[0]], rcond=None)[0]

                    # é‡æ„ä¿¡å·
                    x_reconstructed = D_matrix @ f_est

                    # RMSè¯¯å·®
                    rms = np.sqrt(np.mean(np.abs(x_ref[:len(x_reconstructed)] - x_reconstructed)**2))
                    rms_errors[k_idx, m_idx] = rms

                except:
                    rms_errors[k_idx, m_idx] = np.inf

            if (k_idx + 1) % 2 == 0:
                print(f"  å·²å®Œæˆ K={K}")

        # å¯è§†åŒ–2ï¼šRMSè¯¯å·®çƒ­å›¾
        self._visualize_grid_search(rms_errors, K_range, M_range)

        # é€‰æ‹©æœ€ä¼˜å‚æ•°
        valid_errors = rms_errors[np.isfinite(rms_errors)]
        if len(valid_errors) > 0:
            min_idx = np.unravel_index(np.argmin(rms_errors), rms_errors.shape)
            self.optimal_K = K_range[min_idx[0]]
            self.optimal_M = M_range[min_idx[1]]
            min_rms = rms_errors[min_idx]
        else:
            self.optimal_K = 3
            self.optimal_M = 10
            min_rms = np.inf

        print(f"âœ“ æœ€ä¼˜å‚æ•°ï¼šK={self.optimal_K}, M={self.optimal_M} (RMS={min_rms:.6f})")
        print()

    def _construct_D_matrix(self, d, K, M):
        """æ„å»ºPAè®¾è®¡çŸ©é˜µ D^(M)_N (Fuè®ºæ–‡ Eq.3)"""
        N = len(d) - M
        if N <= 0:
            return np.array([]).reshape(0, (K+1)*(M+1))

        D = np.zeros((N, (K+1)*(M+1)), dtype=complex)

        col_idx = 0
        for m in range(M+1):
            for k in range(K+1):
                # d_n = [d[n], |d[n]|Â²d[n], ..., |d[n]|^{2K}d[n]]
                if m + N <= len(d):
                    D[:, col_idx] = d[m:N+m] * np.abs(d[m:N+m])**(2*k)
                col_idx += 1

        return D

    def _visualize_grid_search(self, rms_errors, K_range, M_range):
        """å¯è§†åŒ–2ï¼šç½‘æ ¼æœç´¢ç»“æœ"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–2ï¼šPAå‚æ•°ç½‘æ ¼æœç´¢")

        plt.figure(figsize=(12, 8))

        # å¤„ç†æ— ç©·å€¼
        rms_plot = rms_errors.copy()
        rms_plot[np.isinf(rms_plot)] = np.nan

        sns.heatmap(rms_plot, annot=False, cmap='hot_r',
                   xticklabels=[str(m) if i % 3 == 0 else '' for i, m in enumerate(M_range)],
                   yticklabels=K_range,
                   cbar_kws={'label': 'RMS è¯¯å·®'})

        plt.xlabel('è®°å¿†æ·±åº¦ M', fontsize=12, fontweight='bold')
        plt.ylabel('éçº¿æ€§é˜¶æ•° K', fontsize=12, fontweight='bold')
        plt.title('PAå‚æ•°ç½‘æ ¼æœç´¢ - RMSè¯¯å·®çƒ­å›¾ï¼ˆåˆ›æ–°ç‚¹1ï¼‰', fontsize=14, fontweight='bold')

        # æ ‡æ³¨æœ€ä¼˜ç‚¹
        if not np.all(np.isnan(rms_plot)):
            min_idx = np.unravel_index(np.nanargmin(rms_plot), rms_plot.shape)
            plt.plot(min_idx[1], min_idx[0], 'c*', markersize=25,
                    markeredgewidth=3, markeredgecolor='white')

        plt.tight_layout()
        plt.savefig('viz_2_grid_search.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_2_grid_search.png\n")

    def optimize_pa_coefficients_dsp(self, lambda_a=0.02, num_iterations=50):
        """
        æ­¥éª¤3ï¼šæ·±åº¦ä¿¡å·å…ˆéªŒä¼˜åŒ–PAç³»æ•°ï¼ˆåˆ›æ–°ç‚¹2 - Zhang et al.ï¼‰
        ä½¿ç”¨HQS + TVæ­£åˆ™åŒ–
        """
        print("=== æ­¥éª¤3ï¼šæ·±åº¦ä¿¡å·å…ˆéªŒä¼˜åŒ–ï¼ˆåˆ›æ–°ç‚¹2ï¼‰===")
        print("æ–¹æ³•ï¼šHQS + TV Regularization (Zhang et al.)")

        # ä¸ºæ¯ä¸ªè®¾å¤‡ä¼°è®¡PAç³»æ•°
        for pos in [self.train_position]:  # åªç”¨è®­ç»ƒä½ç½®
            print(f"å¤„ç†ä½ç½® {pos}")

            for dev_idx, device_id in enumerate(self.device_ids[pos]):
                d_signal = self.all_data[pos][dev_idx]

                # æ„å»ºè®¾è®¡çŸ©é˜µ
                D = self._construct_D_matrix(d_signal, self.optimal_K, self.optimal_M)
                x = d_signal[:D.shape[0]]

                # åˆå§‹åŒ–ï¼ˆLSä¼°è®¡ï¼‰
                f_init = np.linalg.lstsq(D, x, rcond=None)[0]
                f_current = f_init.copy()

                # DSPä¼˜åŒ–
                loss_history = []
                for iter in range(num_iterations):
                    # æ•°æ®ä¿çœŸé¡¹æ¢¯åº¦
                    residual = D @ f_current - x
                    grad_data = D.conj().T @ residual

                    # TVæ­£åˆ™åŒ–æ¢¯åº¦
                    grad_tv = self._compute_tv_gradient(f_current)

                    # æ›´æ–°
                    step_size = 0.001 / (iter + 1)**0.5
                    f_current = f_current - step_size * (grad_data + lambda_a * grad_tv)

                    # æŸå¤±
                    loss = np.linalg.norm(residual)**2 + lambda_a * self._compute_tv_norm(f_current)
                    loss_history.append(loss)

                # ä¿å­˜ä¼˜åŒ–åçš„ç³»æ•°
                self.f_coeffs[device_id] = f_current

            print(f"  âœ“ ä¼˜åŒ–å®Œæˆï¼Œå…± {len(self.f_coeffs)} ä¸ªè®¾å¤‡")

        # å¯è§†åŒ–3ï¼šDSPä¼˜åŒ–è¿‡ç¨‹ï¼ˆç¬¬ä¸€ä¸ªè®¾å¤‡ï¼‰
        first_device = self.device_ids[self.train_position][0]
        self._visualize_dsp_optimization(loss_history, f_init, self.f_coeffs[first_device])

        print()

    def _compute_tv_gradient(self, f):
        """è®¡ç®—TVæ­£åˆ™åŒ–æ¢¯åº¦"""
        grad = np.zeros_like(f)
        eps = 1e-8

        for i in range(len(f) - 1):
            diff = f[i+1] - f[i]
            grad[i] -= diff / (np.abs(diff) + eps)
            grad[i+1] += diff / (np.abs(diff) + eps)

        return grad

    def _compute_tv_norm(self, f):
        """è®¡ç®—TVèŒƒæ•°"""
        return np.sum(np.abs(np.diff(f)))

    def _visualize_dsp_optimization(self, loss_history, f_init, f_opt):
        """å¯è§†åŒ–3ï¼šDSPä¼˜åŒ–è¿‡ç¨‹"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–3ï¼šDSPä¼˜åŒ–è¿‡ç¨‹")

        fig, axes = plt.subplots(1, 3, figsize=(18, 5))

        # å­å›¾1ï¼šæŸå¤±æ›²çº¿
        axes[0].plot(loss_history, linewidth=2, color='#E63946')
        axes[0].set_xlabel('è¿­ä»£æ¬¡æ•°', fontsize=12, fontweight='bold')
        axes[0].set_ylabel('æŸå¤±å€¼', fontsize=12, fontweight='bold')
        axes[0].set_title('DSPä¼˜åŒ–æ”¶æ•›æ›²çº¿', fontsize=13, fontweight='bold')
        axes[0].set_yscale('log')
        axes[0].grid(True, alpha=0.3)

        # å­å›¾2ï¼šç³»æ•°å¹…åº¦å¯¹æ¯”
        x = np.arange(len(f_init))
        width = 0.35
        axes[1].bar(x - width/2, np.abs(f_init), width, label='åˆå§‹LS', alpha=0.7, color='#457B9D')
        axes[1].bar(x + width/2, np.abs(f_opt), width, label='DSPä¼˜åŒ–', alpha=0.7, color='#F4A261')
        axes[1].set_xlabel('ç³»æ•°ç´¢å¼•', fontsize=12, fontweight='bold')
        axes[1].set_ylabel('å¹…åº¦', fontsize=12, fontweight='bold')
        axes[1].set_title('PAç³»æ•°å¹…åº¦å¯¹æ¯”', fontsize=13, fontweight='bold')
        axes[1].legend(fontsize=11)
        axes[1].grid(True, alpha=0.3, axis='y')

        # å­å›¾3ï¼šç³»æ•°ç›¸ä½å¯¹æ¯”
        axes[2].plot(x, np.angle(f_init), 'o-', label='åˆå§‹LS', markersize=5, alpha=0.7, color='#457B9D')
        axes[2].plot(x, np.angle(f_opt), 's-', label='DSPä¼˜åŒ–', markersize=5, alpha=0.7, color='#F4A261')
        axes[2].set_xlabel('ç³»æ•°ç´¢å¼•', fontsize=12, fontweight='bold')
        axes[2].set_ylabel('ç›¸ä½ (rad)', fontsize=12, fontweight='bold')
        axes[2].set_title('PAç³»æ•°ç›¸ä½å¯¹æ¯”', fontsize=13, fontweight='bold')
        axes[2].legend(fontsize=11)
        axes[2].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('viz_3_dsp_optimization.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_3_dsp_optimization.png\n")

    def estimate_channel_multi_regularization(self, lambda_values=None, Lh=10):
        """
        æ­¥éª¤4ï¼šå¤šæ­£åˆ™åŒ–ä¿¡é“ä¼°è®¡ï¼ˆåˆ›æ–°ç‚¹3 - Zhang et al.ï¼‰
        ä½¿ç”¨å¤šä¸ªÎ»è¿›è¡ŒMMSEä¼°è®¡
        """
        print("=== æ­¥éª¤4ï¼šå¤šæ­£åˆ™åŒ–ä¿¡é“ä¼°è®¡ï¼ˆåˆ›æ–°ç‚¹3ï¼‰===")
        print("æ–¹æ³•ï¼šMulti-Î» MMSE (Zhang et al.)")

        if lambda_values is None:
            lambda_values = [0.05, 0.10, 0.15, 0.20, 0.25]

        print(f"æ­£åˆ™åŒ–å‚æ•°ï¼š{lambda_values}")
        print(f"ä¿¡é“é•¿åº¦ï¼šLh={Lh}")

        # ä¸ºæ¯ä¸ªä½ç½®ä¼°è®¡ä¿¡é“
        for pos in self.positions:
            print(f"\nå¤„ç†ä½ç½® {pos}")

            H_estimates = []

            for lambda_h in lambda_values:
                # ä½¿ç”¨ç¬¬ä¸€ä¸ªè®¾å¤‡ä½œä¸ºä»£è¡¨
                y = self.all_data[pos][0]
                d = self.all_data[self.train_position][0]  # å‚è€ƒä¿¡å·

                # æ„å»ºæ‰©å±•è®¾è®¡çŸ©é˜µ D^(M+Lh-1)_N
                D_ext = self._construct_extended_D_matrix(d, self.optimal_K, self.optimal_M, Lh)

                if D_ext.shape[0] == 0:
                    H_estimates.append(np.zeros((Lh, self.optimal_K+1), dtype=complex))
                    continue

                y_trunc = y[:D_ext.shape[0]]

                # MMSEä¼°è®¡ï¼šH = (D'D + Î»I)^(-1) D'y
                DTD = D_ext.conj().T @ D_ext
                reg = lambda_h * np.eye(D_ext.shape[1])

                try:
                    H_est = np.linalg.solve(DTD + reg, D_ext.conj().T @ y_trunc)
                    H_matrix = H_est.reshape(Lh, self.optimal_K+1)
                    H_estimates.append(H_matrix)
                    print(f"  Î»={lambda_h:.2f}: âœ“")
                except:
                    H_estimates.append(np.zeros((Lh, self.optimal_K+1), dtype=complex))
                    print(f"  Î»={lambda_h:.2f}: âœ—")

            # å¹³å‡æ‰€æœ‰ä¼°è®¡
            self.channel_estimates[pos] = np.mean(np.stack(H_estimates), axis=0)

        # å¯è§†åŒ–4ï¼šä¿¡é“é¢‘ç‡å“åº”
        self._visualize_channel_estimates()

        print("\nâœ“ ä¿¡é“ä¼°è®¡å®Œæˆ\n")

    def _construct_extended_D_matrix(self, d, K, M, Lh):
        """æ„å»ºæ‰©å±•è®¾è®¡çŸ©é˜µ D^(M+Lh-1)_N"""
        N = len(d) - M - Lh + 1
        if N <= 0:
            return np.array([]).reshape(0, (K+1)*Lh)

        D_ext = np.zeros((N, (K+1)*Lh), dtype=complex)

        for lh in range(Lh):
            for k in range(K+1):
                col_idx = lh * (K+1) + k
                if lh + N <= len(d):
                    D_ext[:, col_idx] = d[lh:N+lh] * np.abs(d[lh:N+lh])**(2*k)

        return D_ext

    def _visualize_channel_estimates(self):
        """å¯è§†åŒ–4ï¼šä¿¡é“é¢‘ç‡å“åº”"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–4ï¼šä¿¡é“é¢‘ç‡å“åº”å¯¹æ¯”")

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        axes = axes.flatten()

        for idx, pos in enumerate(self.positions):
            if pos not in self.channel_estimates:
                continue

            H_est = self.channel_estimates[pos]

            # è®¡ç®—é¢‘ç‡å“åº”
            H_freq = np.fft.fft(H_est[:, 0], n=512)
            freqs = np.fft.fftfreq(512, d=1.0)

            # åªæ˜¾ç¤ºæ­£é¢‘ç‡
            pos_freqs = freqs[:256]
            H_mag = np.abs(H_freq[:256])

            axes[idx].plot(pos_freqs, 20*np.log10(H_mag + 1e-10),
                          linewidth=2, color='#2A9D8F')
            axes[idx].set_xlabel('å½’ä¸€åŒ–é¢‘ç‡', fontsize=11, fontweight='bold')
            axes[idx].set_ylabel('å¹…åº¦ (dB)', fontsize=11, fontweight='bold')
            axes[idx].set_title(f'ä½ç½® {pos} - ä¿¡é“é¢‘ç‡å“åº”', fontsize=12, fontweight='bold')
            axes[idx].grid(True, alpha=0.3)
            axes[idx].set_xlim([0, 0.5])

        plt.tight_layout()
        plt.savefig('viz_4_channel_frequency_response.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_4_channel_frequency_response.png\n")

    def extract_fu_features(self):
        """
        æ­¥éª¤5ï¼šæå–Fuè®ºæ–‡çš„éçº¿æ€§ç‰¹å¾
        """
        print("=== æ­¥éª¤5ï¼šç‰¹å¾æå–ï¼ˆFu et al. æ ¸å¿ƒç‰¹å¾ï¼‰===")

        for pos in self.positions:
            print(f"æå–ä½ç½® {pos} çš„ç‰¹å¾")

            features = []

            for dev_idx, device_id in enumerate(self.device_ids[pos]):
                # è·å–è¯¥è®¾å¤‡çš„PAç³»æ•°
                if device_id not in self.f_coeffs:
                    # å¦‚æœæ˜¯æµ‹è¯•è®¾å¤‡ï¼Œç”¨è®­ç»ƒé›†ç¬¬ä¸€ä¸ªè®¾å¤‡çš„ç³»æ•°è¿‘ä¼¼
                    f = list(self.f_coeffs.values())[0]
                else:
                    f = self.f_coeffs[device_id]

                # === Fuè®ºæ–‡çš„3ä¸ªä»…å«PAç³»æ•°çš„ç‰¹å¾ ===
                # ç‰¹å¾1: Ï†1 = f1,0 / f3,0
                phi1 = np.abs(f[0] / (f[1] + 1e-10))

                # ç‰¹å¾2: Ï†2 = f1,M / f3,M
                idx_1M = self.optimal_M * (self.optimal_K + 1)
                idx_3M = self.optimal_M * (self.optimal_K + 1) + 1
                phi2 = np.abs(f[idx_1M] / (f[idx_3M] + 1e-10))

                # ç‰¹å¾3: Ï†3 = Î£f1,m / Î£f3,m
                f1_sum = np.sum([f[m*(self.optimal_K+1)] for m in range(self.optimal_M+1)])
                f3_sum = np.sum([f[m*(self.optimal_K+1)+1] for m in range(self.optimal_M+1)])
                phi3 = np.abs(f1_sum / (f3_sum + 1e-10))

                # === Fuè®ºæ–‡çš„æ··åˆç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼‰===
                # ä½¿ç”¨ä¿¡å·ç‰‡æ®µæ„å»º
                y = self.all_data[pos][dev_idx]
                d = self.all_data[self.train_position][0]  # å‚è€ƒè®­ç»ƒç¬¦å·

                # ä½¿ç”¨ä¸åŒé•¿åº¦çš„åºåˆ—
                N1, N2 = 33, 160
                N3, N4 = 33, 320

                try:
                    S_N1N2 = np.sum(y[N1:N2])
                    S_N3N4 = np.sum(y[N3:N4])
                    phi4 = np.abs(S_N1N2 / (S_N3N4 + 1e-10))
                except:
                    phi4 = 0.0

                # æ·»åŠ ä¿¡é“è¡¥å¿åçš„ç‰¹å¾
                if pos in self.channel_estimates:
                    H_est = self.channel_estimates[pos]
                    h_taps = np.mean(H_est, axis=1)

                    phi5 = np.linalg.norm(h_taps, 2)
                    phi6 = np.max(np.abs(h_taps))
                else:
                    phi5 = phi6 = 0.0

                features.append([phi1, phi2, phi3, phi4, phi5, phi6])

            self.features_all[pos] = np.array(features)
            print(f"  âœ“ æå– {len(features)} ä¸ªè®¾å¤‡ï¼Œæ¯ä¸ª 6 ç»´ç‰¹å¾")

        # å¯è§†åŒ–5ï¼šç‰¹å¾åˆ†å¸ƒ
        self._visualize_feature_distribution()

        print("\nâœ“ ç‰¹å¾æå–å®Œæˆ\n")

    def _visualize_feature_distribution(self):
        """å¯è§†åŒ–5ï¼šç‰¹å¾åˆ†å¸ƒå¯¹æ¯”"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–5ï¼šç‰¹å¾åˆ†å¸ƒå¯¹æ¯”")

        fig, axes = plt.subplots(2, 3, figsize=(18, 10))
        axes = axes.flatten()

        feature_names = ['Ï†â‚: fâ‚,â‚€/fâ‚ƒ,â‚€', 'Ï†â‚‚: fâ‚,â‚˜/fâ‚ƒ,â‚˜', 'Ï†â‚ƒ: Î£fâ‚/Î£fâ‚ƒ',
                        'Ï†â‚„: æ··åˆç‰¹å¾', 'Ï†â‚…: ä¿¡é“L2', 'Ï†â‚†: ä¿¡é“å³°å€¼']

        for feat_idx in range(6):
            ax = axes[feat_idx]

            for pos in self.positions:
                if pos not in self.features_all:
                    continue

                features = self.features_all[pos][:, feat_idx]

                ax.hist(features, bins=20, alpha=0.5, label=pos, edgecolor='black')

            ax.set_xlabel('ç‰¹å¾å€¼', fontsize=10, fontweight='bold')
            ax.set_ylabel('é¢‘æ•°', fontsize=10, fontweight='bold')
            ax.set_title(feature_names[feat_idx], fontsize=11, fontweight='bold')
            ax.legend(fontsize=9)
            ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('viz_5_feature_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_5_feature_distribution.png\n")

    def train_classifier(self):
        """æ­¥éª¤6ï¼šè®­ç»ƒåˆ†ç±»å™¨ï¼ˆ1Pè®­ç»ƒï¼‰"""
        print("=== æ­¥éª¤6ï¼šåˆ†ç±»å™¨è®­ç»ƒï¼ˆ1Pï¼šp1è®­ç»ƒï¼‰===")

        # è®­ç»ƒæ•°æ®
        X_train = self.features_all[self.train_position]
        y_train = np.array(self.device_ids[self.train_position])

        print(f"è®­ç»ƒé›†ï¼š{len(X_train)} ä¸ªæ ·æœ¬")
        print(f"è®¾å¤‡IDï¼š{y_train[:5]}...")

        # å½’ä¸€åŒ–
        X_train_norm = self.scaler.fit_transform(X_train)

        # è®­ç»ƒSVM
        print("è®­ç»ƒSVMåˆ†ç±»å™¨...")
        self.classifier = SVC(kernel='rbf', C=10, gamma='scale', random_state=42)
        self.classifier.fit(X_train_norm, y_train)

        print("âœ“ è®­ç»ƒå®Œæˆ\n")

    def evaluate_classifier(self):
        """æ­¥éª¤7ï¼šè¯„ä¼°åˆ†ç±»å™¨ï¼ˆ3Pæµ‹è¯•ï¼šp2, p3, p4ï¼‰"""
        print("=== æ­¥éª¤7ï¼šåˆ†ç±»å™¨è¯„ä¼°ï¼ˆ3Pï¼šp2/p3/p4æµ‹è¯•ï¼‰===")

        results = {}

        for pos in self.test_positions:
            print(f"\næµ‹è¯•ä½ç½®ï¼š{pos}")

            X_test = self.features_all[pos]
            y_test = np.array(self.device_ids[pos])

            # å½’ä¸€åŒ–
            X_test_norm = self.scaler.transform(X_test)

            # é¢„æµ‹
            y_pred = self.classifier.predict(X_test_norm)

            # å‡†ç¡®ç‡
            acc = accuracy_score(y_test, y_pred) * 100

            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_test, y_pred, labels=np.unique(y_test))

            results[pos] = {
                'accuracy': acc,
                'confusion_matrix': cm,
                'y_true': y_test,
                'y_pred': y_pred
            }

            print(f"  å‡†ç¡®ç‡ï¼š{acc:.2f}%")

        # å¯è§†åŒ–6ï¼šå‡†ç¡®ç‡å¯¹æ¯”
        self._visualize_accuracy(results)

        # å¯è§†åŒ–7ï¼šæ··æ·†çŸ©é˜µ
        self._visualize_confusion_matrices(results)

        # æ€»ç»“
        avg_acc = np.mean([r['accuracy'] for r in results.values()])

        print("\n" + "="*70)
        print("å®éªŒæ€»ç»“")
        print("="*70)
        print(f"å¹³å‡å‡†ç¡®ç‡ï¼š{avg_acc:.2f}%")
        for pos, res in results.items():
            print(f"  {pos}: {res['accuracy']:.2f}%")
        print("="*70)

    def _visualize_accuracy(self, results):
        """å¯è§†åŒ–6ï¼šå‡†ç¡®ç‡å¯¹æ¯”"""
        print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–6ï¼šå‡†ç¡®ç‡å¯¹æ¯”")

        plt.figure(figsize=(10, 6))

        positions = list(results.keys())
        accuracies = [results[pos]['accuracy'] for pos in positions]

        bars = plt.bar(range(len(positions)), accuracies,
                      color=['#E63946', '#F4A261', '#2A9D8F'],
                      alpha=0.8, edgecolor='black', linewidth=1.5)

        plt.xlabel('æµ‹è¯•ä½ç½®', fontsize=13, fontweight='bold')
        plt.ylabel('å‡†ç¡®ç‡ (%)', fontsize=13, fontweight='bold')
        plt.title('è·¨ä½ç½®è¯†åˆ«å‡†ç¡®ç‡ï¼ˆ1Pè®­ç»ƒï¼šp1 â†’ 3Pæµ‹è¯•ï¼šp2/p3/p4ï¼‰',
                 fontsize=14, fontweight='bold')
        plt.xticks(range(len(positions)), positions, fontsize=12)
        plt.ylim([0, 105])
        plt.grid(True, alpha=0.3, axis='y')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, acc) in enumerate(zip(bars, accuracies)):
            plt.text(i, acc + 2, f'{acc:.1f}%',
                    ha='center', va='bottom', fontsize=12, fontweight='bold')

        # æ·»åŠ å¹³å‡çº¿
        avg_acc = np.mean(accuracies)
        plt.axhline(y=avg_acc, color='red', linestyle='--', linewidth=2,
                   label=f'å¹³å‡: {avg_acc:.2f}%', alpha=0.7)
        plt.legend(fontsize=11)

        plt.tight_layout()
        plt.savefig('viz_6_accuracy_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_6_accuracy_comparison.png")

    def _visualize_confusion_matrices(self, results):
        """å¯è§†åŒ–7ï¼šæ··æ·†çŸ©é˜µ"""
        print("ğŸ¨ ç”Ÿæˆå¯è§†åŒ–7ï¼šæ··æ·†çŸ©é˜µ")

        fig, axes = plt.subplots(1, 3, figsize=(20, 6))

        for idx, (pos, res) in enumerate(results.items()):
            cm = res['confusion_matrix']

            # å½’ä¸€åŒ–
            cm_norm = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-10)

            im = axes[idx].imshow(cm_norm, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')

            axes[idx].set_xlabel('é¢„æµ‹è®¾å¤‡ID', fontsize=11, fontweight='bold')
            axes[idx].set_ylabel('çœŸå®è®¾å¤‡ID', fontsize=11, fontweight='bold')
            axes[idx].set_title(f'{pos} - æ··æ·†çŸ©é˜µ (å‡†ç¡®ç‡: {res["accuracy"]:.1f}%)',
                               fontsize=12, fontweight='bold')

            # é¢œè‰²æ¡
            cbar = plt.colorbar(im, ax=axes[idx])
            cbar.set_label('å‡†ç¡®ç‡', fontsize=10)

            # è®¾ç½®åˆ»åº¦
            device_ids = np.unique(res['y_true'])
            axes[idx].set_xticks(range(len(device_ids)))
            axes[idx].set_yticks(range(len(device_ids)))
            axes[idx].set_xticklabels(device_ids, rotation=45, fontsize=8)
            axes[idx].set_yticklabels(device_ids, fontsize=8)

        plt.tight_layout()
        plt.savefig('viz_7_confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.close()
        print("  âœ“ ä¿å­˜: viz_7_confusion_matrices.png\n")

    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´æµç¨‹"""
        start_time = time.time()

        # æ­¥éª¤1ï¼šåŠ è½½æ•°æ®
        self.load_data()

        # æ­¥éª¤2ï¼šè‡ªé€‚åº”PAå‚æ•°ä¼°è®¡ï¼ˆåˆ›æ–°ç‚¹1ï¼‰
        self.estimate_pa_parameters_grid_search()

        # æ­¥éª¤3ï¼šæ·±åº¦ä¿¡å·å…ˆéªŒä¼˜åŒ–ï¼ˆåˆ›æ–°ç‚¹2ï¼‰
        self.optimize_pa_coefficients_dsp()

        # æ­¥éª¤4ï¼šå¤šæ­£åˆ™åŒ–ä¿¡é“ä¼°è®¡ï¼ˆåˆ›æ–°ç‚¹3ï¼‰
        self.estimate_channel_multi_regularization()

        # æ­¥éª¤5ï¼šç‰¹å¾æå–
        self.extract_fu_features()

        # æ­¥éª¤6ï¼šè®­ç»ƒåˆ†ç±»å™¨
        self.train_classifier()

        # æ­¥éª¤7ï¼šè¯„ä¼°
        self.evaluate_classifier()

        elapsed = time.time() - start_time
        print(f"\nâ±ï¸ æ€»è€—æ—¶ï¼š{elapsed:.2f} ç§’")
        print("\nâœ… æ‰€æœ‰å¯è§†åŒ–å·²ä¿å­˜ï¼š")
        print("  1. viz_1_raw_signals.png - åŸå§‹ä¿¡å·å¯¹æ¯”")
        print("  2. viz_2_grid_search.png - PAå‚æ•°ç½‘æ ¼æœç´¢")
        print("  3. viz_3_dsp_optimization.png - DSPä¼˜åŒ–è¿‡ç¨‹")
        print("  4. viz_4_channel_frequency_response.png - ä¿¡é“é¢‘ç‡å“åº”")
        print("  5. viz_5_feature_distribution.png - ç‰¹å¾åˆ†å¸ƒ")
        print("  6. viz_6_accuracy_comparison.png - å‡†ç¡®ç‡å¯¹æ¯”")
        print("  7. viz_7_confusion_matrices.png - æ··æ·†çŸ©é˜µ")

def main():
    """ä¸»å‡½æ•°"""
    system = ChannelResilientRFF()
    system.run_full_pipeline()

if __name__ == "__main__":
    main()
